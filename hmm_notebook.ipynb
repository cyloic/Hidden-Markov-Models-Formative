{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c71d5dd",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "\n",
    "This notebook focuses on recognizing human activities — **jumping**, **standing**, **walking**, and **staying still** — using motion data collected from an iPhone’s **accelerometer** and **gyroscope** sensors.\n",
    "\n",
    "Each dataset contains time-series readings of linear acceleration and angular velocity along three axes. We preprocess the data, extract **time-domain** (mean, variance, correlation) and **frequency-domain** (dominant frequency, spectral energy) features, and use these as observation vectors for a **Hidden Markov Model (HMM)**.\n",
    "\n",
    "The HMM is designed to learn how activities transition over time and to decode the most probable sequence of actions using the **Viterbi algorithm**, while its parameters are trained via the **Baum–Welch algorithm**.\n",
    "\n",
    "Finally, we evaluate the model’s accuracy and ability to generalize to unseen recordings, and reflect on the strengths and weaknesses of the approach.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecdf22e",
   "metadata": {},
   "source": [
    "### 1. Imports and Configuration\n",
    "\n",
    "We import essential Python libraries like NumPy, pandas, SciPy, Matplotlib, and hmmlearn.\n",
    "Basic notebook configurations for visualization and reproducibility are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb3427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import fftpack\n",
    "from scipy.stats import zscore, multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hmmlearn import hmm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8d8b1",
   "metadata": {},
   "source": [
    "### 2. Data Loading\n",
    "We load all the pre-organized sensor files for each activity category.\n",
    "Each dataset includes accelerometer and gyroscope readings with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDERS = {\n",
    "    'jump': 'datasets/final_jump_data',\n",
    "    'walking': 'datasets/final_walking_data',\n",
    "    'stand': 'datasets/final_stand_data',\n",
    "    'still': 'datasets/final_still_data'\n",
    "}\n",
    "FS = 100  # sampling frequency (Hz)\n",
    "WINDOW_SEC = 1.0  # 1-second windows\n",
    "WINDOW_STEP = 0.5  # seconds (50% overlap)\n",
    "N_PCA = 10  # optional dimensionality reduction for HMM emissions\n",
    "N_COMPONENTS_HMM = 4  # 4 hidden states (jump, walking, stand, still)\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb0e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 102 sessions\n"
     ]
    }
   ],
   "source": [
    "def load_all_sessions(data_folders):\n",
    "    \"\"\"Load CSVs from labeled folders. Returns list of dicts: {'activity', 'file', 'df'}\"\"\"\n",
    "    sessions = []\n",
    "    for label, folder in data_folders.items():\n",
    "        files = sorted(glob.glob(os.path.join(folder, '*.csv')))\n",
    "        for f in files:\n",
    "            df = pd.read_csv(f)\n",
    "            # Ensure expected columns\n",
    "            expected = ['seconds_elapsed','ax','ay','az','gx','gy','gz']\n",
    "            if not all(c in df.columns for c in expected):\n",
    "                raise ValueError(f\"File {f} missing expected columns. Found: {df.columns.tolist()}\")\n",
    "            sessions.append({'activity': label, 'file': f, 'df': df[expected].copy()})\n",
    "    return sessions\n",
    "\n",
    "sessions = load_all_sessions(DATA_FOLDERS)\n",
    "print(f\"Loaded {len(sessions)} sessions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae2971",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "We clean and normalize data to ensure consistent scale and quality.\n",
    "Noise filtering and axis alignment are applied to prepare signals for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346018ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(df, window_sec=WINDOW_SEC, step_sec=WINDOW_STEP, fs=FS):\n",
    "    n_win = int(round(window_sec * fs))\n",
    "    step = int(round(step_sec * fs))\n",
    "    data = df[['ax','ay','az','gx','gy','gz']].values\n",
    "    starts = np.arange(0, len(df) - n_win + 1, step)\n",
    "    windows = []\n",
    "    for s in starts:\n",
    "        win = data[s:s+n_win]\n",
    "        windows.append({'start_idx': s, 'window': win, 'seconds': df['seconds_elapsed'].iloc[s]})\n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f0228",
   "metadata": {},
   "source": [
    "### 4. Feature Extraction\n",
    "\n",
    "We compute statistical (mean, std, variance, SMA) and frequency features (FFT peaks, spectral energy).\n",
    "These features summarize motion characteristics over time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_features(win):\n",
    "    # win shape: (n_samples, 6) -> ax,ay,az,gx,gy,gz\n",
    "    feats = {}\n",
    "    accel = win[:,0:3]\n",
    "    gyro = win[:,3:6]\n",
    "    # Time-domain: per-axis mean, std, var, rms, ptp\n",
    "    for i, name in enumerate(['ax','ay','az']):\n",
    "        x = accel[:,i]\n",
    "        feats[f'{name}_mean'] = np.mean(x)\n",
    "        feats[f'{name}_std'] = np.std(x)\n",
    "        feats[f'{name}_var'] = np.var(x)\n",
    "        feats[f'{name}_rms'] = np.sqrt(np.mean(x**2))\n",
    "        feats[f'{name}_ptp'] = np.ptp(x)\n",
    "    for i, name in enumerate(['gx','gy','gz']):\n",
    "        x = gyro[:,i]\n",
    "        feats[f'{name}_mean'] = np.mean(x)\n",
    "        feats[f'{name}_std'] = np.std(x)\n",
    "        feats[f'{name}_var'] = np.var(x)\n",
    "        feats[f'{name}_rms'] = np.sqrt(np.mean(x**2))\n",
    "        feats[f'{name}_ptp'] = np.ptp(x)\n",
    "    # SMA for accel\n",
    "    feats['acc_sma'] = np.sum(np.abs(accel)) / accel.shape[0]\n",
    "    # Correlations between axes (accel)\n",
    "    feats['acc_corr_xy'] = np.corrcoef(accel[:,0], accel[:,1])[0,1]\n",
    "    feats['acc_corr_xz'] = np.corrcoef(accel[:,0], accel[:,2])[0,1]\n",
    "    feats['acc_corr_yz'] = np.corrcoef(accel[:,1], accel[:,2])[0,1]\n",
    "    return feats\n",
    "\n",
    "def compute_freq_features(win, fs=FS):\n",
    "    feats = {}\n",
    "    n = win.shape[0]\n",
    "    accel = win[:,0:3]\n",
    "    gyro = win[:,3:6]\n",
    "    # For each axis compute dominant frequency and spectral energy\n",
    "    for i, name in enumerate(['ax','ay','az']):\n",
    "        x = accel[:,i]\n",
    "        X = np.fft.rfft(x)\n",
    "        P = np.abs(X)**2\n",
    "        freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "        dominant = freqs[np.argmax(P)]\n",
    "        feats[f'{name}_domfreq'] = dominant\n",
    "        feats[f'{name}_specenergy'] = np.sum(P)\n",
    "    for i, name in enumerate(['gx','gy','gz']):\n",
    "        x = gyro[:,i]\n",
    "        X = np.fft.rfft(x)\n",
    "        P = np.abs(X)**2\n",
    "        freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "        dominant = freqs[np.argmax(P)]\n",
    "        feats[f'{name}_domfreq'] = dominant\n",
    "        feats[f'{name}_specenergy'] = np.sum(P)\n",
    "    return feats\n",
    "\n",
    "def extract_features_from_windows(windows):\n",
    "    feat_list = []\n",
    "    for w in windows:\n",
    "        win = w['window']\n",
    "        tfeats = compute_time_features(win)\n",
    "        ffeats = compute_freq_features(win)\n",
    "        merged = {**tfeats, **ffeats}\n",
    "        merged['start_idx'] = w['start_idx']\n",
    "        merged['seconds'] = w['seconds']\n",
    "        feat_list.append(merged)\n",
    "    return pd.DataFrame(feat_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86752fa",
   "metadata": {},
   "source": [
    "### 5. Feature Aggregation and Labeling & Train-Test Split\n",
    "\n",
    "Extracted features from all files are concatenated and labeled by activity type.\n",
    "This creates a structured dataset for HMM training and evaluation.\n",
    "We split the dataset into training and unseen test sets to ensure fair evaluation.\n",
    "Data balance across activities is checked to prevent bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe99ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for 102 sessions\n",
      "Total windows: 1270\n"
     ]
    }
   ],
   "source": [
    "all_feature_sessions = []  # list of dicts with: activity, file, features_df\n",
    "for s in sessions:\n",
    "    windows = sliding_windows(s['df'])\n",
    "    feats = extract_features_from_windows(windows)\n",
    "    feats['activity'] = s['activity']\n",
    "    feats['file'] = os.path.basename(s['file'])\n",
    "    all_feature_sessions.append({'activity': s['activity'], 'file': s['file'], 'features': feats})\n",
    "\n",
    "print('Extracted features for', len(all_feature_sessions), 'sessions')\n",
    "\n",
    "# Concatenate to a single DataFrame for convenience\n",
    "features_df = pd.concat([fs['features'] for fs in all_feature_sessions], ignore_index=True)\n",
    "print('Total windows:', len(features_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbde74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features -> PCA dims: (1270, 10)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns not used as features\n",
    "non_feature_cols = ['start_idx','seconds','activity','file']\n",
    "feature_cols = [c for c in features_df.columns if c not in non_feature_cols]\n",
    "X_raw = features_df[feature_cols].fillna(0).values\n",
    "# Z-score normalize across the dataset\n",
    "X_norm = zscore(X_raw, axis=0)\n",
    "# Optional PCA to reduce dimensionality for HMM emissions\n",
    "pca = PCA(n_components=min(N_PCA, X_norm.shape[1]), random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "print('Features -> PCA dims:', X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(fs['features']) for fs in all_feature_sessions]\n",
    "# order of concatenation matches all_feature_sessions\n",
    "X_concat = np.vstack([fs['features'][feature_cols].fillna(0).values for fs in all_feature_sessions])\n",
    "X_concat_norm = zscore(X_concat, axis=0)\n",
    "X_concat_pca = pca.transform(X_concat_norm)\n",
    "\n",
    "# Keep labels per window for evaluation\n",
    "labels_concat = np.concatenate([fs['features']['activity'].values for fs in all_feature_sessions])\n",
    "filenames_concat = np.concatenate([np.repeat(os.path.basename(fs['file']), len(fs['features'])) for fs in all_feature_sessions])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
